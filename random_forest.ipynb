{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiyue02/5052_final_project/blob/main/random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a9a8756",
      "metadata": {
        "id": "3a9a8756"
      },
      "source": [
        "# Netflix User Churn Prediction Project\n",
        "\n",
        "## Objective\n",
        "Predict whether a user will remain active (`is_active`) using recent watch patterns, engagement metrics, demographics, and subscription plan information. Identify the most influential drivers of churn.\n",
        "\n",
        "## Dataset Overview\n",
        "- **Source**: Netflix 2025: User Behavior Dataset (210K+ Records)\n",
        "- **Target Variable**: `is_active` (binary) from users.csv\n",
        "- **Features**: Demographics, subscription info, watch patterns, engagement metrics\n",
        "- **Data Files**: users.csv, movies.csv, watch_history.csv, recommendation_logs.csv, search_logs.csv, reviews.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "d533f3bb",
      "metadata": {
        "id": "d533f3bb",
        "outputId": "6ee4e725-f953-4751-80ff-dc27964d88ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ML_model\n"
          ]
        }
      ],
      "source": [
        "# Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%cd /content/ML_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f013a8c",
      "metadata": {
        "id": "2f013a8c"
      },
      "source": [
        "## 1. Data Loading and Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f8fad1",
      "metadata": {
        "id": "09f8fad1",
        "outputId": "42875ac7-2b3a-4b4e-81d2-4cfc73e78ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shapes:\n",
            "Users: (10300, 16)\n",
            "Movies: (1040, 18)\n",
            "Watch History: (105000, 12)\n",
            "Recommendation Logs: (52000, 11)\n",
            "Search Logs: (26500, 11)\n",
            "Reviews: (15450, 12)\n",
            "\n",
            "================================================================================\n",
            "Users Table Sample (Target: is_active):\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      user_id                      email first_name last_name   age  gender  \\\n",
              "0  user_00001   figueroajohn@example.org      Erica     Garza  43.0    Male   \n",
              "1  user_00002      blakeerik@example.com     Joshua   Bernard  38.0    Male   \n",
              "2  user_00003        smiller@example.net    Barbara  Williams  32.0  Female   \n",
              "3  user_00004  mitchellclark@example.com    Chelsea  Ferguson  11.0    Male   \n",
              "4  user_00005      richard13@example.net      Jason    Foster  21.0  Female   \n",
              "\n",
              "  country state_province                city subscription_plan  \\\n",
              "0     USA  Massachusetts  North Jefferyhaven             Basic   \n",
              "1     USA          Texas      North Noahstad          Premium+   \n",
              "2     USA       Michigan          Traciebury          Standard   \n",
              "3     USA           Ohio          South Noah          Standard   \n",
              "4     USA        Arizona         West Donald          Standard   \n",
              "\n",
              "  subscription_start_date  is_active  monthly_spend primary_device  \\\n",
              "0              2024-04-08       True          36.06         Laptop   \n",
              "1              2024-05-24       True          14.59        Desktop   \n",
              "2              2023-09-22      False          11.71        Desktop   \n",
              "3              2024-08-21       True          28.56         Laptop   \n",
              "4              2024-10-28       True           9.54        Desktop   \n",
              "\n",
              "   household_size                  created_at  \n",
              "0             1.0  2023-04-01 14:40:50.540242  \n",
              "1             2.0  2024-10-10 15:39:11.030515  \n",
              "2             3.0  2024-06-29 14:27:49.560875  \n",
              "3             2.0  2023-04-11 01:01:59.614841  \n",
              "4             6.0  2025-04-12 19:59:30.137806  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1831b7f0-99b7-4e9d-b51c-b8db435fbeb1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>email</th>\n",
              "      <th>first_name</th>\n",
              "      <th>last_name</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>country</th>\n",
              "      <th>state_province</th>\n",
              "      <th>city</th>\n",
              "      <th>subscription_plan</th>\n",
              "      <th>subscription_start_date</th>\n",
              "      <th>is_active</th>\n",
              "      <th>monthly_spend</th>\n",
              "      <th>primary_device</th>\n",
              "      <th>household_size</th>\n",
              "      <th>created_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_00001</td>\n",
              "      <td>figueroajohn@example.org</td>\n",
              "      <td>Erica</td>\n",
              "      <td>Garza</td>\n",
              "      <td>43.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>USA</td>\n",
              "      <td>Massachusetts</td>\n",
              "      <td>North Jefferyhaven</td>\n",
              "      <td>Basic</td>\n",
              "      <td>2024-04-08</td>\n",
              "      <td>True</td>\n",
              "      <td>36.06</td>\n",
              "      <td>Laptop</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023-04-01 14:40:50.540242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_00002</td>\n",
              "      <td>blakeerik@example.com</td>\n",
              "      <td>Joshua</td>\n",
              "      <td>Bernard</td>\n",
              "      <td>38.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>USA</td>\n",
              "      <td>Texas</td>\n",
              "      <td>North Noahstad</td>\n",
              "      <td>Premium+</td>\n",
              "      <td>2024-05-24</td>\n",
              "      <td>True</td>\n",
              "      <td>14.59</td>\n",
              "      <td>Desktop</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2024-10-10 15:39:11.030515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_00003</td>\n",
              "      <td>smiller@example.net</td>\n",
              "      <td>Barbara</td>\n",
              "      <td>Williams</td>\n",
              "      <td>32.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>USA</td>\n",
              "      <td>Michigan</td>\n",
              "      <td>Traciebury</td>\n",
              "      <td>Standard</td>\n",
              "      <td>2023-09-22</td>\n",
              "      <td>False</td>\n",
              "      <td>11.71</td>\n",
              "      <td>Desktop</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2024-06-29 14:27:49.560875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_00004</td>\n",
              "      <td>mitchellclark@example.com</td>\n",
              "      <td>Chelsea</td>\n",
              "      <td>Ferguson</td>\n",
              "      <td>11.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>USA</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>South Noah</td>\n",
              "      <td>Standard</td>\n",
              "      <td>2024-08-21</td>\n",
              "      <td>True</td>\n",
              "      <td>28.56</td>\n",
              "      <td>Laptop</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2023-04-11 01:01:59.614841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_00005</td>\n",
              "      <td>richard13@example.net</td>\n",
              "      <td>Jason</td>\n",
              "      <td>Foster</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>USA</td>\n",
              "      <td>Arizona</td>\n",
              "      <td>West Donald</td>\n",
              "      <td>Standard</td>\n",
              "      <td>2024-10-28</td>\n",
              "      <td>True</td>\n",
              "      <td>9.54</td>\n",
              "      <td>Desktop</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2025-04-12 19:59:30.137806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1831b7f0-99b7-4e9d-b51c-b8db435fbeb1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1831b7f0-99b7-4e9d-b51c-b8db435fbeb1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1831b7f0-99b7-4e9d-b51c-b8db435fbeb1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dc8579a7-50ea-4522-891b-20ffd5a4599f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc8579a7-50ea-4522-891b-20ffd5a4599f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dc8579a7-50ea-4522-891b-20ffd5a4599f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "users",
              "summary": "{\n  \"name\": \"users\",\n  \"rows\": 10300,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"user_06253\",\n          \"user_04685\",\n          \"user_01732\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9789,\n        \"samples\": [\n          \"gonzalezstephen@example.org\",\n          \"snyderjoshua@example.net\",\n          \"kleinbrent@example.net\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"first_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 651,\n        \"samples\": [\n          \"Lucas\",\n          \"Rita\",\n          \"Trevor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 989,\n        \"samples\": [\n          \"Long\",\n          \"Burton\",\n          \"Espinoza\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.580667476668067,\n        \"min\": -7.0,\n        \"max\": 109.0,\n        \"num_unique_values\": 97,\n        \"samples\": [\n          62.0,\n          101.0,\n          -5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Female\",\n          \"Prefer not to say\",\n          \"Male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Canada\",\n          \"USA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state_province\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Virginia\",\n          \"British Columbia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"city\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7762,\n        \"samples\": [\n          \"New Roberttown\",\n          \"Angelside\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subscription_plan\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Premium+\",\n          \"Premium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subscription_start_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1096,\n        \"samples\": [\n          \"2025-05-21\",\n          \"2024-01-10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_active\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"monthly_spend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 65.72382399018804,\n        \"min\": 0.11,\n        \"max\": 997.8,\n        \"num_unique_values\": 3488,\n        \"samples\": [\n          36.25,\n          13.53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"primary_device\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Laptop\",\n          \"Desktop\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"household_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.563250906968099,\n        \"min\": 1.0,\n        \"max\": 8.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.0,\n          7.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"2022-08-20 05:29:19.512816\",\n          \"2025-06-24 06:45:53.207004\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Load all datasets\n",
        "users = pd.read_csv('archive/users.csv')\n",
        "movies = pd.read_csv('archive/movies.csv')\n",
        "watch_history = pd.read_csv('archive/watch_history.csv')\n",
        "recommendation_logs = pd.read_csv('archive/recommendation_logs.csv')\n",
        "search_logs = pd.read_csv('archive/search_logs.csv')\n",
        "reviews = pd.read_csv('archive/reviews.csv')\n",
        "\n",
        "print(\"Dataset Shapes:\")\n",
        "print(f\"Users: {users.shape}\")\n",
        "print(f\"Movies: {movies.shape}\")\n",
        "print(f\"Watch History: {watch_history.shape}\")\n",
        "print(f\"Recommendation Logs: {recommendation_logs.shape}\")\n",
        "print(f\"Search Logs: {search_logs.shape}\")\n",
        "print(f\"Reviews: {reviews.shape}\")\n",
        "\n",
        "# Display first few rows of users table (contains target variable)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Users Table Sample (Target: is_active):\")\n",
        "print(\"=\"*80)\n",
        "users.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652eb1e3",
      "metadata": {
        "id": "652eb1e3"
      },
      "outputs": [],
      "source": [
        "# Check data info and target distribution\n",
        "print(\"Users Table Info:\")\n",
        "print(users.info())\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Target Variable Distribution:\")\n",
        "print(\"=\"*80)\n",
        "target_dist = users['is_active'].value_counts()\n",
        "print(target_dist)\n",
        "print(f\"\\nChurn Rate: {(1 - target_dist[True] / len(users)) * 100:.2f}%\")\n",
        "\n",
        "# Visualize target distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "users['is_active'].value_counts().plot(kind='bar', ax=axes[0], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[0].set_title('User Activity Status Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Is Active')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Inactive', 'Active'], rotation=0)\n",
        "\n",
        "# Pie chart\n",
        "colors = ['#ff6b6b', '#4ecdc4']\n",
        "axes[1].pie(target_dist, labels=['Active', 'Inactive'], autopct='%1.1f%%',\n",
        "            colors=colors, startangle=90)\n",
        "axes[1].set_title('User Activity Proportion', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b36cbb",
      "metadata": {
        "id": "e4b36cbb"
      },
      "source": [
        "## 2. Data Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48f8b484",
      "metadata": {
        "id": "48f8b484"
      },
      "outputs": [],
      "source": [
        "# Check for missing values and duplicates\n",
        "print(\"=\"*80)\n",
        "print(\"DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. MISSING VALUES:\")\n",
        "print(\"-\" * 80)\n",
        "missing_users = users.isnull().sum()\n",
        "missing_pct = (missing_users / len(users) * 100).round(2)\n",
        "missing_df = pd.DataFrame({'Missing Count': missing_users, 'Percentage': missing_pct})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "print(missing_df)\n",
        "\n",
        "print(\"\\n2. DUPLICATE RECORDS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Users duplicates: {users.duplicated().sum()} ({users.duplicated().sum() / len(users) * 100:.2f}%)\")\n",
        "print(f\"Watch history duplicates: {watch_history.duplicated().sum()} ({watch_history.duplicated().sum() / len(watch_history) * 100:.2f}%)\")\n",
        "\n",
        "print(\"\\n3. DATA TYPES:\")\n",
        "print(\"-\" * 80)\n",
        "print(users.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae1a09b",
      "metadata": {
        "id": "4ae1a09b"
      },
      "outputs": [],
      "source": [
        "# Visualize missing data patterns\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Missing values heatmap\n",
        "ax = axes[0]\n",
        "missing_matrix = users.isnull()\n",
        "if missing_matrix.sum().sum() > 0:\n",
        "    sns.heatmap(missing_matrix, cbar=True, yticklabels=False, cmap='YlOrRd', ax=ax)\n",
        "    ax.set_title('Missing Values Pattern (Yellow = Missing)', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Features')\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14)\n",
        "    ax.set_title('Missing Values Pattern', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Missing percentage by column\n",
        "ax = axes[1]\n",
        "if missing_df.shape[0] > 0:\n",
        "    ax.barh(missing_df.index, missing_df['Percentage'], color='coral', alpha=0.7)\n",
        "    ax.set_xlabel('Missing Percentage (%)', fontsize=11)\n",
        "    ax.set_title('Missing Values by Feature', fontsize=12, fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14)\n",
        "    ax.set_title('Missing Values by Feature', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b0abc58",
      "metadata": {
        "id": "4b0abc58"
      },
      "source": [
        "### Handling Duplicates Strategy\n",
        "\n",
        "**Order of Operations**: Handle duplicates BEFORE missing value imputation to avoid artificially inflating patterns.\n",
        "\n",
        "Duplicate records can arise from:\n",
        "- **Data collection errors**: Same user recorded multiple times\n",
        "- **System issues**: Double recording of transactions\n",
        "- **Natural duplicates**: Multiple people with same characteristics\n",
        "\n",
        "**Strategy**:\n",
        "- Identify duplicates based on unique identifier (user_id)\n",
        "- Keep first occurrence (most recent or complete record)\n",
        "- Remove exact duplicates across all columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b9a7d7",
      "metadata": {
        "id": "97b9a7d7"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Handle duplicates FIRST\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 1: DUPLICATE HANDLING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check for duplicates based on user_id (should be unique)\n",
        "print(\"\\n1. USER_ID DUPLICATES:\")\n",
        "user_id_duplicates = users[users.duplicated(subset=['user_id'], keep=False)]\n",
        "print(f\"   Records with duplicate user_id: {len(user_id_duplicates)}\")\n",
        "\n",
        "users_clean = users.copy()\n",
        "\n",
        "if len(user_id_duplicates) > 0:\n",
        "    print(f\"   Unique users with duplicates: {user_id_duplicates['user_id'].nunique()}\")\n",
        "    # Keep first occurrence\n",
        "    users_clean = users_clean.drop_duplicates(subset=['user_id'], keep='first')\n",
        "    print(f\"   Strategy: Keep first occurrence\")\n",
        "    print(f\"   Removed: {len(users) - len(users_clean)} records\")\n",
        "else:\n",
        "    print(f\"   No user_id duplicates found!\")\n",
        "\n",
        "# Check for exact duplicates (all columns identical)\n",
        "print(\"\\n2. EXACT DUPLICATES (all columns):\")\n",
        "exact_duplicates = users_clean.duplicated()\n",
        "print(f\"   Exact duplicate records: {exact_duplicates.sum()}\")\n",
        "\n",
        "if exact_duplicates.sum() > 0:\n",
        "    users_clean = users_clean.drop_duplicates(keep='first')\n",
        "    print(f\"   Strategy: Keep first occurrence\")\n",
        "    print(f\"   Removed: {exact_duplicates.sum()} records\")\n",
        "else:\n",
        "    print(f\"   No exact duplicates found!\")\n",
        "\n",
        "# Handle duplicates in watch_history\n",
        "print(\"\\n3. WATCH HISTORY DUPLICATES:\")\n",
        "watch_dup = watch_history.duplicated()\n",
        "print(f\"   Exact duplicate sessions: {watch_dup.sum()} ({watch_dup.sum()/len(watch_history)*100:.2f}%)\")\n",
        "\n",
        "if watch_dup.sum() > 0:\n",
        "    watch_history_clean = watch_history.drop_duplicates(keep='first')\n",
        "    print(f\"   Strategy: Remove exact duplicates, keep first\")\n",
        "    print(f\"   Removed: {watch_dup.sum()} records\")\n",
        "    watch_history = watch_history_clean\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Users before:        {len(users)}\")\n",
        "print(f\"Users after:         {len(users_clean)}\")\n",
        "print(f\"Records removed:     {len(users) - len(users_clean)}\")\n",
        "print(f\"Data retention rate: {len(users_clean)/len(users)*100:.2f}%\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Update users dataframe for next step\n",
        "users = users_clean\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: MISSING VALUE IMPUTATION STRATEGY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Age - DO NOT CHANGE (keep missing as is or will be handled later)\n",
        "if users['age'].isnull().sum() > 0:\n",
        "    print(f\"\\n1. AGE:\")\n",
        "    print(f\"   Missing: {users['age'].isnull().sum()} ({users['age'].isnull().sum()/len(users)*100:.2f}%)\")\n",
        "    print(f\"   Strategy: NO CHANGE - Keep missing values as is\")\n",
        "\n",
        "# 2. Gender - categorical, use 'Prefer not to say'\n",
        "if users['gender'].isnull().sum() > 0:\n",
        "    print(f\"\\n2. GENDER:\")\n",
        "    print(f\"   Missing: {users['gender'].isnull().sum()} ({users['gender'].isnull().sum()/len(users)*100:.2f}%)\")\n",
        "    print(f\"   Strategy: Fill with 'Prefer not to say' category\")\n",
        "    users['gender'] = users['gender'].fillna('Prefer not to say')\n",
        "\n",
        "# 3. Monthly spend - 0 for inactive accounts (assuming free users or inactive)\n",
        "if users['monthly_spend'].isnull().sum() > 0:\n",
        "    print(f\"\\n3. MONTHLY SPEND:\")\n",
        "    print(f\"   Missing: {users['monthly_spend'].isnull().sum()} ({users['monthly_spend'].isnull().sum()/len(users)*100:.2f}%)\")\n",
        "    print(f\"   Strategy: Fill with 0 (assuming free users or inactive accounts)\")\n",
        "    users['monthly_spend'] = users['monthly_spend'].fillna(0)\n",
        "\n",
        "# 4. Household size - use median by country\n",
        "if users['household_size'].isnull().sum() > 0:\n",
        "    print(f\"\\n4. HOUSEHOLD SIZE:\")\n",
        "    print(f\"   Missing: {users['household_size'].isnull().sum()} ({users['household_size'].isnull().sum()/len(users)*100:.2f}%)\")\n",
        "    print(f\"   Strategy: Fill with median by country\")\n",
        "\n",
        "    # Calculate median household size by country\n",
        "    country_medians = users.groupby('country')['household_size'].median()\n",
        "    print(f\"   Countries with data: {len(country_medians)}\")\n",
        "\n",
        "    # Fill missing values with country-specific median (fallback to overall median if country median is NaN)\n",
        "    users['household_size'] = users.apply(\n",
        "        lambda row: country_medians.get(row['country'], users['household_size'].median())\n",
        "        if pd.isnull(row['household_size']) else row['household_size'],\n",
        "        axis=1\n",
        "    )\n",
        "    print(f\"   Applied country-specific medians\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"IMPUTATION COMPLETE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total missing values after imputation: {users.isnull().sum().sum()}\")\n",
        "if users.isnull().sum().sum() > 0:\n",
        "    print(f\"\\nRemaining missing values by column:\")\n",
        "    print(users.isnull().sum()[users.isnull().sum() > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1490c982",
      "metadata": {
        "id": "1490c982"
      },
      "source": [
        "### Handling Outliers\n",
        "\n",
        "Outliers can significantly impact model performance. Common approaches:\n",
        "- **Capping/Clipping**: Set min/max thresholds (e.g., age 18-100)\n",
        "- **IQR Method**: Remove/cap values beyond Q1 - 1.5×IQR or Q3 + 1.5×IQR\n",
        "- **Z-score**: Remove values with |z-score| > 3\n",
        "- **Domain knowledge**: Use business logic (e.g., monthly spend caps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89187246",
      "metadata": {
        "id": "89187246"
      },
      "outputs": [],
      "source": [
        "# Identify and handle outliers\n",
        "print(\"=\"*80)\n",
        "print(\"OUTLIER DETECTION AND TREATMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Function to detect outliers using IQR method\n",
        "def detect_outliers_iqr(data, column, multiplier=1.5):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - multiplier * IQR\n",
        "    upper_bound = Q3 + multiplier * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "# 1. Age outliers\n",
        "print(\"\\n1. AGE OUTLIERS:\")\n",
        "age_outliers, age_lower, age_upper = detect_outliers_iqr(users, 'age')\n",
        "print(f\"   Outliers detected: {len(age_outliers)} ({len(age_outliers)/len(users)*100:.2f}%)\")\n",
        "print(f\"   IQR bounds: [{age_lower:.1f}, {age_upper:.1f}]\")\n",
        "print(f\"   Actual range: [{users['age'].min():.1f}, {users['age'].max():.1f}]\")\n",
        "print(f\"   Strategy: Cap at [18, 100] based on domain knowledge\")\n",
        "users['age'] = users['age'].clip(18, 100)\n",
        "print(f\"   After capping: [{users['age'].min():.1f}, {users['age'].max():.1f}]\")\n",
        "\n",
        "# 2. Monthly spend outliers\n",
        "print(\"\\n2. MONTHLY SPEND OUTLIERS:\")\n",
        "spend_outliers, spend_lower, spend_upper = detect_outliers_iqr(users, 'monthly_spend', multiplier=3)\n",
        "print(f\"   Outliers detected: {len(spend_outliers)} ({len(spend_outliers)/len(users)*100:.2f}%)\")\n",
        "print(f\"   IQR bounds (3×IQR): [${spend_lower:.2f}, ${spend_upper:.2f}]\")\n",
        "print(f\"   Actual range: [${users['monthly_spend'].min():.2f}, ${users['monthly_spend'].max():.2f}]\")\n",
        "print(f\"   Strategy: Cap at 3×IQR bounds (conservative approach)\")\n",
        "users['monthly_spend'] = users['monthly_spend'].clip(spend_lower, spend_upper)\n",
        "print(f\"   After capping: [${users['monthly_spend'].min():.2f}, ${users['monthly_spend'].max():.2f}]\")\n",
        "\n",
        "# 3. Household size outliers\n",
        "print(\"\\n3. HOUSEHOLD SIZE OUTLIERS:\")\n",
        "household_outliers, household_lower, household_upper = detect_outliers_iqr(users, 'household_size')\n",
        "print(f\"   Outliers detected: {len(household_outliers)} ({len(household_outliers)/len(users)*100:.2f}%)\")\n",
        "print(f\"   IQR bounds: [{household_lower:.1f}, {household_upper:.1f}]\")\n",
        "print(f\"   Actual range: [{users['household_size'].min():.0f}, {users['household_size'].max():.0f}]\")\n",
        "print(f\"   Strategy: Cap at reasonable limit [1, 10]\")\n",
        "users['household_size'] = users['household_size'].clip(1, 10)\n",
        "print(f\"   After capping: [{users['household_size'].min():.0f}, {users['household_size'].max():.0f}]\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"OUTLIER TREATMENT COMPLETE\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592e252e",
      "metadata": {
        "id": "592e252e"
      },
      "outputs": [],
      "source": [
        "# Visualize data distributions before and after cleaning\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Age distribution\n",
        "ax = axes[0, 0]\n",
        "ax.hist(users['age'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Age', fontsize=11)\n",
        "ax.set_ylabel('Frequency', fontsize=11)\n",
        "ax.set_title('Age Distribution (After Cleaning)', fontsize=12, fontweight='bold')\n",
        "ax.axvline(users['age'].median(), color='red', linestyle='--', label=f\"Median: {users['age'].median():.1f}\")\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Monthly spend distribution\n",
        "ax = axes[0, 1]\n",
        "ax.hist(users['monthly_spend'], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Monthly Spend ($)', fontsize=11)\n",
        "ax.set_ylabel('Frequency', fontsize=11)\n",
        "ax.set_title('Monthly Spend Distribution (After Cleaning)', fontsize=12, fontweight='bold')\n",
        "ax.axvline(users['monthly_spend'].median(), color='red', linestyle='--',\n",
        "          label=f\"Median: ${users['monthly_spend'].median():.2f}\")\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Household size distribution\n",
        "ax = axes[0, 2]\n",
        "household_counts = users['household_size'].value_counts().sort_index()\n",
        "ax.bar(household_counts.index, household_counts.values, color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Household Size', fontsize=11)\n",
        "ax.set_ylabel('Count', fontsize=11)\n",
        "ax.set_title('Household Size Distribution (After Cleaning)', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Gender distribution\n",
        "ax = axes[1, 0]\n",
        "gender_counts = users['gender'].value_counts()\n",
        "ax.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=90,\n",
        "       colors=plt.cm.Set3(range(len(gender_counts))))\n",
        "ax.set_title('Gender Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Subscription plan distribution\n",
        "ax = axes[1, 1]\n",
        "plan_counts = users['subscription_plan'].value_counts()\n",
        "ax.bar(range(len(plan_counts)), plan_counts.values, color='purple', alpha=0.6, edgecolor='black')\n",
        "ax.set_xticks(range(len(plan_counts)))\n",
        "ax.set_xticklabels(plan_counts.index, rotation=45, ha='right')\n",
        "ax.set_xlabel('Subscription Plan', fontsize=11)\n",
        "ax.set_ylabel('Count', fontsize=11)\n",
        "ax.set_title('Subscription Plan Distribution', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Active vs Inactive users\n",
        "ax = axes[1, 2]\n",
        "active_counts = users['is_active'].value_counts()\n",
        "colors_active = ['#ff6b6b', '#4ecdc4']\n",
        "bars = ax.bar(['Inactive', 'Active'], [active_counts[False], active_counts[True]],\n",
        "              color=colors_active, alpha=0.7, edgecolor='black')\n",
        "ax.set_ylabel('Count', fontsize=11)\n",
        "ax.set_title('User Activity Status (Target)', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}\\n({height/len(users)*100:.1f}%)',\n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA QUALITY SUMMARY AFTER CLEANING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total users: {len(users)}\")\n",
        "print(f\"Total features: {users.shape[1]}\")\n",
        "print(f\"Missing values: {users.isnull().sum().sum()}\")\n",
        "print(f\"Duplicate records: {users.duplicated().sum()}\")\n",
        "print(f\"\\nData is ready for feature engineering!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fa110ce",
      "metadata": {
        "id": "5fa110ce"
      },
      "source": [
        "## 3. Feature Engineering - Create Engagement Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee62be65",
      "metadata": {
        "id": "ee62be65"
      },
      "outputs": [],
      "source": [
        "# Create user-level engagement features from watch history\n",
        "print(\"Creating engagement features from watch history...\")\n",
        "\n",
        "# Aggregate watch history per user\n",
        "watch_features = watch_history.groupby('user_id').agg({\n",
        "    'session_id': 'count',  # Total sessions\n",
        "    'watch_duration_minutes': ['sum', 'mean', 'std'],  # Watch duration stats\n",
        "    'progress_percentage': ['mean', 'std'],  # Completion stats\n",
        "    'is_download': 'sum',  # Downloads count\n",
        "    'user_rating': 'count'  # Number of ratings given\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten column names\n",
        "watch_features.columns = ['user_id', 'total_sessions', 'total_watch_minutes',\n",
        "                         'avg_watch_minutes', 'std_watch_minutes',\n",
        "                         'avg_progress', 'std_progress', 'total_downloads', 'total_ratings']\n",
        "\n",
        "# Fill NaN std values with 0 (for users with single session)\n",
        "watch_features['std_watch_minutes'] = watch_features['std_watch_minutes'].fillna(0)\n",
        "watch_features['std_progress'] = watch_features['std_progress'].fillna(0)\n",
        "\n",
        "# Count unique actions per user\n",
        "action_counts = watch_history.groupby(['user_id', 'action']).size().unstack(fill_value=0).reset_index()\n",
        "action_counts.columns = ['user_id'] + [f'action_{col}' for col in action_counts.columns[1:]]\n",
        "\n",
        "# Merge action counts\n",
        "watch_features = watch_features.merge(action_counts, on='user_id', how='left')\n",
        "\n",
        "print(f\"Watch features shape: {watch_features.shape}\")\n",
        "print(\"\\nSample watch features:\")\n",
        "watch_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f43f9c91",
      "metadata": {
        "id": "f43f9c91"
      },
      "outputs": [],
      "source": [
        "# Create recommendation engagement features\n",
        "print(\"Creating recommendation features...\")\n",
        "\n",
        "rec_features = recommendation_logs.groupby('user_id').agg({\n",
        "    'recommendation_id': 'count',  # Total recommendations\n",
        "    'was_clicked': 'sum'  # Total clicks (was_clicked is boolean, sum gives count of True)\n",
        "}).reset_index()\n",
        "\n",
        "rec_features.columns = ['user_id', 'total_recommendations', 'total_clicks']\n",
        "\n",
        "# Calculate click-through rate (avoid division by zero)\n",
        "rec_features['click_through_rate'] = rec_features.apply(\n",
        "    lambda row: row['total_clicks'] / row['total_recommendations'] if row['total_recommendations'] > 0 else 0,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"Recommendation features shape: {rec_features.shape}\")\n",
        "print(f\"Sample recommendation features:\")\n",
        "print(rec_features.head())\n",
        "\n",
        "# Create search engagement features\n",
        "print(\"\\nCreating search features...\")\n",
        "\n",
        "search_features = search_logs.groupby('user_id').agg({\n",
        "    'search_id': 'count',  # Total searches\n",
        "    'results_returned': 'mean',  # Average results\n",
        "    'search_query': lambda x: x.str.len().mean()  # Average query length\n",
        "}).reset_index()\n",
        "\n",
        "search_features.columns = ['user_id', 'total_searches', 'avg_results', 'avg_query_length']\n",
        "\n",
        "print(f\"Search features shape: {search_features.shape}\")\n",
        "print(f\"Sample search features:\")\n",
        "print(search_features.head())\n",
        "\n",
        "# Create review engagement features\n",
        "print(\"\\nCreating review features...\")\n",
        "\n",
        "review_features = reviews.groupby('user_id').agg({\n",
        "    'review_id': 'count',  # Total reviews\n",
        "    'rating': 'mean',  # Average rating given\n",
        "    'helpful_votes': 'sum'  # Total helpful votes\n",
        "}).reset_index()\n",
        "\n",
        "review_features.columns = ['user_id', 'total_reviews', 'avg_rating_given', 'total_helpful_votes']\n",
        "\n",
        "print(f\"Review features shape: {review_features.shape}\")\n",
        "print(f\"Sample review features:\")\n",
        "print(review_features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf7056c",
      "metadata": {
        "id": "2cf7056c"
      },
      "source": [
        "## 4. Merge Features and Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf5df20",
      "metadata": {
        "id": "2bf5df20"
      },
      "source": [
        "### Feature Summary and Visualization\n",
        "\n",
        "Let's examine all the features we've created from different data sources and visualize their distributions and statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee90190c",
      "metadata": {
        "id": "ee90190c"
      },
      "outputs": [],
      "source": [
        "# Feature Summary Tables\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Watch History Features\n",
        "print(\"\\n1. WATCH HISTORY FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Number of features: {len(watch_features.columns) - 1}\")  # excluding user_id\n",
        "print(f\"Users with watch history: {len(watch_features)}\")\n",
        "print(\"\\nFeature list:\")\n",
        "for col in watch_features.columns[1:]:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\nWatch Features Statistics:\")\n",
        "watch_stats = watch_features.drop('user_id', axis=1).describe().round(2)\n",
        "print(watch_stats)\n",
        "\n",
        "# 2. Recommendation Features\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2. RECOMMENDATION FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Number of features: {len(rec_features.columns) - 1}\")\n",
        "print(f\"Users with recommendations: {len(rec_features)}\")\n",
        "print(\"\\nFeature list:\")\n",
        "for col in rec_features.columns[1:]:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\nRecommendation Features Statistics:\")\n",
        "rec_stats = rec_features.drop('user_id', axis=1).describe().round(2)\n",
        "print(rec_stats)\n",
        "\n",
        "# 3. Search Features\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3. SEARCH FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Number of features: {len(search_features.columns) - 1}\")\n",
        "print(f\"Users with search history: {len(search_features)}\")\n",
        "print(\"\\nFeature list:\")\n",
        "for col in search_features.columns[1:]:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\nSearch Features Statistics:\")\n",
        "search_stats = search_features.drop('user_id', axis=1).describe().round(2)\n",
        "print(search_stats)\n",
        "\n",
        "# 4. Review Features\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"4. REVIEW FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Number of features: {len(review_features.columns) - 1}\")\n",
        "print(f\"Users with reviews: {len(review_features)}\")\n",
        "print(\"\\nFeature list:\")\n",
        "for col in review_features.columns[1:]:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\nReview Features Statistics:\")\n",
        "review_stats = review_features.drop('user_id', axis=1).describe().round(2)\n",
        "print(review_stats)\n",
        "\n",
        "# Overall summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OVERALL FEATURE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "total_features = (len(watch_features.columns) - 1 + len(rec_features.columns) - 1 +\n",
        "                  len(search_features.columns) - 1 + len(review_features.columns) - 1)\n",
        "print(f\"Total engineered features: {total_features}\")\n",
        "print(f\"  - Watch history: {len(watch_features.columns) - 1}\")\n",
        "print(f\"  - Recommendations: {len(rec_features.columns) - 1}\")\n",
        "print(f\"  - Search: {len(search_features.columns) - 1}\")\n",
        "print(f\"  - Reviews: {len(review_features.columns) - 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e379324a",
      "metadata": {
        "id": "e379324a"
      },
      "outputs": [],
      "source": [
        "# Visualize feature distributions\n",
        "fig, axes = plt.subplots(3, 4, figsize=(18, 14))\n",
        "fig.suptitle('Engineered Features Distribution', fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# Watch History Features (top 4)\n",
        "ax = axes[0, 0]\n",
        "ax.hist(watch_features['total_sessions'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Total Sessions', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Watch: Total Sessions', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "ax = axes[0, 1]\n",
        "ax.hist(watch_features['total_watch_minutes'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Total Watch Minutes', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Watch: Total Minutes', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "ax = axes[0, 2]\n",
        "ax.hist(watch_features['avg_progress'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Average Progress (%)', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Watch: Avg Progress', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "ax = axes[0, 3]\n",
        "ax.hist(watch_features['total_downloads'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Total Downloads', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Watch: Downloads', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Recommendation Features\n",
        "ax = axes[1, 0]\n",
        "ax.hist(rec_features['total_recommendations'], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Total Recommendations', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Rec: Total Recommendations', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "ax = axes[1, 1]\n",
        "ax.hist(rec_features['total_clicks'], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Total Clicks', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Rec: Total Clicks', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "ax = axes[1, 2]\n",
        "ax.hist(rec_features['click_through_rate'], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Click-Through Rate', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Rec: Click-Through Rate', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Search Features\n",
        "ax = axes[1, 3]\n",
        "ax.hist(search_features['total_searches'], bins=30, color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Total Searches', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Search: Total Searches', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "ax = axes[2, 0]\n",
        "ax.hist(search_features['avg_results'], bins=30, color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Avg Results', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Search: Avg Results', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "ax = axes[2, 1]\n",
        "ax.hist(search_features['avg_query_length'], bins=30, color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Avg Query Length', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Search: Avg Query Length', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Review Features\n",
        "ax = axes[2, 2]\n",
        "ax.hist(review_features['total_reviews'], bins=30, color='mediumpurple', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Total Reviews', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Review: Total Reviews', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "ax = axes[2, 3]\n",
        "ax.hist(review_features['avg_rating_given'], bins=30, color='mediumpurple', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Avg Rating Given', fontsize=10)\n",
        "ax.set_ylabel('Frequency', fontsize=10)\n",
        "ax.set_title('Review: Avg Rating', fontsize=11, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af4f875",
      "metadata": {
        "id": "5af4f875"
      },
      "outputs": [],
      "source": [
        "# Feature correlation heatmap (top features)\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select key features for correlation analysis\n",
        "key_features = pd.DataFrame()\n",
        "key_features['total_sessions'] = watch_features['total_sessions']\n",
        "key_features['total_watch_minutes'] = watch_features['total_watch_minutes']\n",
        "key_features['avg_progress'] = watch_features['avg_progress']\n",
        "key_features['total_downloads'] = watch_features['total_downloads']\n",
        "key_features['total_recommendations'] = rec_features['total_recommendations']\n",
        "key_features['click_through_rate'] = rec_features['click_through_rate']\n",
        "key_features['total_searches'] = search_features['total_searches']\n",
        "key_features['total_reviews'] = review_features['total_reviews']\n",
        "key_features['avg_rating_given'] = review_features['avg_rating_given']\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = key_features.corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
        "ax.set_title('Feature Correlation Heatmap (Key Features)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print top correlations\n",
        "print(\"\\nTop Positive Correlations:\")\n",
        "print(\"-\" * 80)\n",
        "# Get upper triangle of correlation matrix\n",
        "upper_triangle = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "# Stack and sort\n",
        "correlations = upper_triangle.stack().sort_values(ascending=False)\n",
        "print(correlations.head(10))\n",
        "\n",
        "print(\"\\nTop Negative Correlations:\")\n",
        "print(\"-\" * 80)\n",
        "print(correlations.tail(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a7d1a2",
      "metadata": {
        "id": "96a7d1a2"
      },
      "outputs": [],
      "source": [
        "# Feature comparison by user activity status\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE COMPARISON: ACTIVE vs INACTIVE USERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Merge features with user activity status for comparison\n",
        "feature_comparison = users[['user_id', 'is_active']].copy()\n",
        "feature_comparison = feature_comparison.merge(watch_features, on='user_id', how='left')\n",
        "feature_comparison = feature_comparison.merge(rec_features, on='user_id', how='left')\n",
        "feature_comparison = feature_comparison.merge(search_features, on='user_id', how='left')\n",
        "feature_comparison = feature_comparison.merge(review_features, on='user_id', how='left')\n",
        "\n",
        "# Fill NaN with 0 for users with no activity\n",
        "feature_comparison = feature_comparison.fillna(0)\n",
        "\n",
        "# Compare key metrics between active and inactive users\n",
        "print(\"\\nKey Metrics Comparison:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "comparison_metrics = ['total_sessions', 'total_watch_minutes', 'avg_progress',\n",
        "                     'total_recommendations', 'click_through_rate',\n",
        "                     'total_searches', 'total_reviews', 'avg_rating_given']\n",
        "\n",
        "comparison_table = []\n",
        "for metric in comparison_metrics:\n",
        "    if metric in feature_comparison.columns:\n",
        "        active_mean = feature_comparison[feature_comparison['is_active'] == True][metric].mean()\n",
        "        inactive_mean = feature_comparison[feature_comparison['is_active'] == False][metric].mean()\n",
        "        difference = active_mean - inactive_mean\n",
        "        pct_diff = (difference / inactive_mean * 100) if inactive_mean != 0 else 0\n",
        "\n",
        "        comparison_table.append({\n",
        "            'Feature': metric,\n",
        "            'Active (mean)': f'{active_mean:.2f}',\n",
        "            'Inactive (mean)': f'{inactive_mean:.2f}',\n",
        "            'Difference': f'{difference:.2f}',\n",
        "            'Change (%)': f'{pct_diff:.1f}%'\n",
        "        })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_table)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
        "fig.suptitle('Feature Comparison: Active vs Inactive Users', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, metric in enumerate(comparison_metrics[:8]):\n",
        "    row = idx // 4\n",
        "    col = idx % 4\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    if metric in feature_comparison.columns:\n",
        "        active_data = feature_comparison[feature_comparison['is_active'] == True][metric]\n",
        "        inactive_data = feature_comparison[feature_comparison['is_active'] == False][metric]\n",
        "\n",
        "        # Box plot\n",
        "        box_data = [active_data, inactive_data]\n",
        "        bp = ax.boxplot(box_data, labels=['Active', 'Inactive'], patch_artist=True)\n",
        "        bp['boxes'][0].set_facecolor('#4ecdc4')\n",
        "        bp['boxes'][1].set_facecolor('#ff6b6b')\n",
        "\n",
        "        ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=9)\n",
        "        ax.set_title(metric.replace('_', ' ').title(), fontsize=10, fontweight='bold')\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe66614",
      "metadata": {
        "id": "0fe66614"
      },
      "outputs": [],
      "source": [
        "# Merge all features with users table\n",
        "print(\"Merging all features...\")\n",
        "\n",
        "df = users.copy()\n",
        "df = df.merge(watch_features, on='user_id', how='left')\n",
        "df = df.merge(rec_features, on='user_id', how='left')\n",
        "df = df.merge(search_features, on='user_id', how='left')\n",
        "df = df.merge(review_features, on='user_id', how='left')\n",
        "\n",
        "print(f\"Merged dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "\n",
        "# Handle missing values from merges (users with no activity in certain tables)\n",
        "print(\"\\nFilling missing values from left joins with 0...\")\n",
        "activity_columns = watch_features.columns.tolist()[1:] + rec_features.columns.tolist()[1:] + \\\n",
        "                   search_features.columns.tolist()[1:] + review_features.columns.tolist()[1:]\n",
        "\n",
        "for col in activity_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "print(f\"\\nMissing values after filling activity metrics:\")\n",
        "print(df.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5810a5ee",
      "metadata": {
        "id": "5810a5ee"
      },
      "outputs": [],
      "source": [
        "# Prepare features for modeling\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE PREPARATION FOR MODELING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select features for modeling\n",
        "# Exclude: user_id, email, names, dates, text fields\n",
        "exclude_cols = ['user_id', 'email', 'first_name', 'last_name', 'subscription_start_date', 'created_at']\n",
        "\n",
        "# Separate target variable\n",
        "y = df['is_active'].astype(int)\n",
        "X = df.drop(columns=exclude_cols + ['is_active'])\n",
        "\n",
        "print(f\"\\nTarget variable shape: {y.shape}\")\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"\\nFeatures to be used ({len(X.columns)}):\")\n",
        "print(X.columns.tolist())\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
        "print(f\"Numerical features ({len(numerical_cols)}): {len(numerical_cols)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c88baca",
      "metadata": {
        "id": "5c88baca"
      },
      "outputs": [],
      "source": [
        "# Check categorical values before encoding\n",
        "print(\"=\"*80)\n",
        "print(\"CATEGORICAL FEATURE VALUES CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col.upper()}:\")\n",
        "    print(\"-\" * 80)\n",
        "    value_counts = X[col].value_counts()\n",
        "    print(f\"Total unique values: {len(value_counts)}\")\n",
        "    print(f\"Value distribution:\")\n",
        "    print(value_counts)\n",
        "\n",
        "\n",
        "# Check for any unusual values\n",
        "print(f\"\\nGender value lengths:\")\n",
        "for val in X['gender'].unique():\n",
        "    print(f\"  '{val}': length = {len(str(val))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a689a8",
      "metadata": {
        "id": "58a689a8"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "print(\"Encoding categorical variables...\")\n",
        "\n",
        "label_encoders = {}\n",
        "X_encoded = X.copy()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "    print(f\"  {col}: {len(le.classes_)} unique values\")\n",
        "\n",
        "print(f\"\\nEncoded features shape: {X_encoded.shape}\")\n",
        "print(\"\\nSample of encoded data:\")\n",
        "X_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26197729",
      "metadata": {
        "id": "26197729"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values before train-test split\n",
        "print(\"=\"*80)\n",
        "print(\"DATA VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nChecking for NaN values in encoded features...\")\n",
        "nan_count = X_encoded.isnull().sum().sum()\n",
        "print(f\"Total NaN values: {nan_count}\")\n",
        "\n",
        "if nan_count > 0:\n",
        "    print(\"\\nNaN values found per column:\")\n",
        "    nan_cols = X_encoded.isnull().sum()[X_encoded.isnull().sum() > 0]\n",
        "    print(nan_cols)\n",
        "    print(\"\\nFilling NaN values with 0 (after encoding, NaN typically means missing category)...\")\n",
        "    X_encoded = X_encoded.fillna(0)\n",
        "    print(\"NaN values filled successfully.\")\n",
        "else:\n",
        "    print(\"No NaN values found. Data is clean.\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "\n",
        "# Train-test split\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"\\nTraining set target distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTest set target distribution:\")\n",
        "print(y_test.value_counts())\n",
        "\n",
        "# Store number of features for mtry calculations\n",
        "n_features = X_train.shape[1]\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Total number of features (p): {n_features}\")\n",
        "print(f\"  - sqrt(p) = {int(np.sqrt(n_features))}\")\n",
        "print(f\"  - p/3 ≈ {int(n_features/3)}\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e76d104a",
      "metadata": {
        "id": "e76d104a"
      },
      "source": [
        "## 5. Random Forest with Different m Values\n",
        "\n",
        "We'll train Random Forest models with three different values of m (max_features):\n",
        "1. **m = sqrt(p)**: Default for classification (most common)\n",
        "2. **m = p/2 = 15**: Half of the features at each split\n",
        "3. **m = p = 30** (Bagging): Uses all features at each split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de6dd55e",
      "metadata": {
        "id": "de6dd55e"
      },
      "outputs": [],
      "source": [
        "# Define different m values to test\n",
        "m_values = {\n",
        "    'm = sqrt(p)': int(np.sqrt(n_features)),\n",
        "    'm = p/2': n_features // 2,  # p/2 = 15\n",
        "    'm = p (Bagging)': n_features  # p = 30\n",
        "}\n",
        "\n",
        "# Store results\n",
        "rf_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING RANDOM FOREST MODELS WITH DIFFERENT m VALUES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for name, m_val in m_values.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training Random Forest with {name} = {m_val}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Train Random Forest\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=500,\n",
        "        max_features=m_val,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2\n",
        "    )\n",
        "\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = rf_model.predict(X_train)\n",
        "    y_test_pred = rf_model.predict(X_test)\n",
        "    y_test_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    train_error = 1 - train_accuracy\n",
        "    test_error = 1 - test_accuracy\n",
        "    auc_score = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "    # Store results\n",
        "    rf_results[name] = {\n",
        "        'model': rf_model,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'train_error': train_error,\n",
        "        'test_error': test_error,\n",
        "        'auc': auc_score,\n",
        "        'predictions': y_test_pred,\n",
        "        'probabilities': y_test_proba,\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_test_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Training Error:    {train_error:.4f}\")\n",
        "    print(f\"  Test Accuracy:     {test_accuracy:.4f}\")\n",
        "    print(f\"  Test Error:        {test_error:.4f}\")\n",
        "    print(f\"  AUC Score:         {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"All Random Forest models trained successfully!\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc160abd",
      "metadata": {
        "id": "fc160abd"
      },
      "outputs": [],
      "source": [
        "# Compare Random Forest models\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(rf_results.keys()),\n",
        "    'm_value': [m_values[k] for k in rf_results.keys()],\n",
        "    'Train_Accuracy': [rf_results[k]['train_accuracy'] for k in rf_results.keys()],\n",
        "    'Test_Accuracy': [rf_results[k]['test_accuracy'] for k in rf_results.keys()],\n",
        "    'Train_Error': [rf_results[k]['train_error'] for k in rf_results.keys()],\n",
        "    'Test_Error': [rf_results[k]['test_error'] for k in rf_results.keys()],\n",
        "    'AUC': [rf_results[k]['auc'] for k in rf_results.keys()]\n",
        "})\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RANDOM FOREST MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_name = comparison_df.loc[comparison_df['Test_Error'].idxmin(), 'Model']\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Best Random Forest Model: {best_model_name}\")\n",
        "print(f\"  Test Error: {rf_results[best_model_name]['test_error']:.4f}\")\n",
        "print(f\"  Test Accuracy: {rf_results[best_model_name]['test_accuracy']:.4f}\")\n",
        "print(f\"  AUC: {rf_results[best_model_name]['auc']:.4f}\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6f6700",
      "metadata": {
        "id": "6b6f6700"
      },
      "outputs": [],
      "source": [
        "# Feature Importance - Random Forest Models Comparison\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE IMPORTANCE COMPARISON ACROSS RANDOM FOREST MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get feature importance from all Random Forest models\n",
        "model_names = list(rf_results.keys())\n",
        "n_models = len(model_names)\n",
        "\n",
        "# Create subplot layout: 1x3 for 3 models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "fig.suptitle('Random Forest: Feature Importance Comparison for Different m Values',\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "colors = ['steelblue', 'forestgreen', 'coral']\n",
        "\n",
        "for idx, name in enumerate(model_names):\n",
        "    ax = axes[idx]\n",
        "    model = rf_results[name]['model']\n",
        "\n",
        "    # Get feature importance\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Plot top 15 features\n",
        "    top_15 = importance_df.head(15)\n",
        "    ax.barh(range(len(top_15)), top_15['Importance'], color=colors[idx], alpha=0.8)\n",
        "    ax.set_yticks(range(len(top_15)))\n",
        "    ax.set_yticklabels(top_15['Feature'], fontsize=9)\n",
        "    ax.set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'{name}\\nTop 15 Features', fontsize=12, fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Print top 5 for this model\n",
        "    print(f\"\\n{name} - Top 5 Features:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(importance_df.head(5).to_string(index=False))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c497b12d",
      "metadata": {
        "id": "c497b12d"
      },
      "outputs": [],
      "source": [
        "# Visualize Random Forest comparison - Confusion Matrices for all m values\n",
        "model_names = list(rf_results.keys())\n",
        "n_models = len(model_names)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Random Forest: Confusion Matrices for Different m Values',\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "colors_maps = ['Blues', 'Greens', 'Oranges']\n",
        "\n",
        "for idx, name in enumerate(model_names):\n",
        "    ax = axes[idx]\n",
        "    cm = rf_results[name]['confusion_matrix']\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=colors_maps[idx], ax=ax,\n",
        "                cbar=False, annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
        "\n",
        "    # Labels and title\n",
        "    ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'{name}\\nTest Error: {rf_results[name][\"test_error\"]:.4f} | AUC: {rf_results[name][\"auc\"]:.4f}',\n",
        "                fontsize=12, fontweight='bold', pad=10)\n",
        "    ax.set_xticklabels(['Inactive', 'Active'], fontsize=11)\n",
        "    ax.set_yticklabels(['Inactive', 'Active'], fontsize=11, rotation=0)\n",
        "\n",
        "    # Calculate and display accuracy metrics\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    # Add text box with metrics\n",
        "    textstr = f'Accuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}'\n",
        "    props = dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray')\n",
        "    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,\n",
        "            verticalalignment='top', bbox=props)\n",
        "\n",
        "plt.show()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96806f34",
      "metadata": {
        "id": "96806f34"
      },
      "outputs": [],
      "source": [
        "# test for different number of trees\n",
        "# Test Classification Error vs Number of Trees\n",
        "print(\"=\"*80)\n",
        "print(\"RANDOM FOREST: TEST ERROR vs NUMBER OF TREES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train models with increasing number of trees to track error progression\n",
        "n_tree_values = [10, 25, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "error_progression = {name: [] for name in m_values.keys()}\n",
        "\n",
        "print(\"\\nTraining Random Forest models with varying tree counts...\")\n",
        "print(\"This may take a moment...\\n\")\n",
        "\n",
        "for n_trees in n_tree_values:\n",
        "    print(f\"Training with {n_trees} trees...\", end=' ')\n",
        "    for name, m_val in m_values.items():\n",
        "        # Train model with current number of trees\n",
        "        rf_temp = RandomForestClassifier(\n",
        "            n_estimators=n_trees,\n",
        "            max_features=m_val,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2\n",
        "        )\n",
        "        rf_temp.fit(X_train, y_train)\n",
        "\n",
        "        # Calculate test error\n",
        "        y_pred_temp = rf_temp.predict(X_test)\n",
        "        test_error = 1 - accuracy_score(y_test, y_pred_temp)\n",
        "        error_progression[name].append(test_error)\n",
        "    print(\"✓\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Visualize error progression\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
        "\n",
        "colors_line = ['#ff6b6b', '#4ecdc4', '#45b7d1']\n",
        "markers = ['o', 's', 'D']\n",
        "\n",
        "for idx, (name, errors) in enumerate(error_progression.items()):\n",
        "    ax.plot(n_tree_values, errors,\n",
        "            marker=markers[idx],\n",
        "            linewidth=2.5,\n",
        "            markersize=8,\n",
        "            label=name,\n",
        "            color=colors_line[idx],\n",
        "            alpha=0.8)\n",
        "\n",
        "    # Annotate final error value\n",
        "    final_error = errors[-1]\n",
        "    ax.annotate(f'{final_error:.4f}',\n",
        "                xy=(n_tree_values[-1], final_error),\n",
        "                xytext=(10, 0),\n",
        "                textcoords='offset points',\n",
        "                fontsize=9,\n",
        "                fontweight='bold',\n",
        "                color=colors_line[idx])\n",
        "\n",
        "ax.set_xlabel('Number of Trees', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Test Classification Error', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Random Forest: Test Error vs Number of Trees\\nfor Different m Values',\n",
        "             fontsize=15, fontweight='bold', pad=20)\n",
        "ax.legend(fontsize=11, loc='upper right', framealpha=0.9)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "ax.set_xlim([0, 520])\n",
        "\n",
        "# Add horizontal line at each model's final error\n",
        "for idx, (name, errors) in enumerate(error_progression.items()):\n",
        "    ax.axhline(y=errors[-1], color=colors_line[idx], linestyle=':', alpha=0.3, linewidth=1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST ERROR PROGRESSION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "summary_data = []\n",
        "for name, errors in error_progression.items():\n",
        "    summary_data.append({\n",
        "        'Model': name,\n",
        "        'Error @10 trees': f'{errors[0]:.4f}',\n",
        "        'Error @100 trees': f'{errors[3]:.4f}',\n",
        "        'Error @500 trees': f'{errors[-1]:.4f}',\n",
        "        'Improvement': f'{(errors[0] - errors[-1])*100:.2f}%'\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY OBSERVATIONS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"- Error decreases as number of trees increases (diminishing returns)\")\n",
        "print(\"- Most improvement occurs in first 100-200 trees\")\n",
        "print(\"- All models stabilize around 300-400 trees\")\n",
        "print(\"- Different m values show different convergence patterns\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "281f9507",
      "metadata": {
        "id": "281f9507"
      },
      "source": [
        "## 6. Gradient Boosting with Different Tree Depths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b33458",
      "metadata": {
        "id": "a1b33458"
      },
      "outputs": [],
      "source": [
        "# Train Gradient Boosting models with different max_depth values\n",
        "# Focusing on depths 1, 2\n",
        "depths = [1, 2]\n",
        "gb_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING GRADIENT BOOSTING MODELS WITH DIFFERENT DEPTHS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for depth in depths:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training Gradient Boosting with max_depth = {depth}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Train Gradient Boosting\n",
        "    gb_model = GradientBoostingClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=depth,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        subsample=0.8,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2\n",
        "    )\n",
        "\n",
        "    gb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = gb_model.predict(X_train)\n",
        "    y_test_pred = gb_model.predict(X_test)\n",
        "    y_test_proba = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    train_error = 1 - train_accuracy\n",
        "    test_error = 1 - test_accuracy\n",
        "    auc_score = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "    # Store results\n",
        "    gb_results[depth] = {\n",
        "        'model': gb_model,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'train_error': train_error,\n",
        "        'test_error': test_error,\n",
        "        'auc': auc_score,\n",
        "        'predictions': y_test_pred,\n",
        "        'probabilities': y_test_proba,\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_test_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Training Error:    {train_error:.4f}\")\n",
        "    print(f\"  Test Accuracy:     {test_accuracy:.4f}\")\n",
        "    print(f\"  Test Error:        {test_error:.4f}\")\n",
        "    print(f\"  AUC Score:         {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"All Gradient Boosting models trained successfully!\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c69c664",
      "metadata": {
        "id": "9c69c664"
      },
      "outputs": [],
      "source": [
        "# Compare Gradient Boosting models\n",
        "gb_comparison_df = pd.DataFrame({\n",
        "    'Max_Depth': list(gb_results.keys()),\n",
        "    'Train_Accuracy': [gb_results[k]['train_accuracy'] for k in gb_results.keys()],\n",
        "    'Test_Accuracy': [gb_results[k]['test_accuracy'] for k in gb_results.keys()],\n",
        "    'Train_Error': [gb_results[k]['train_error'] for k in gb_results.keys()],\n",
        "    'Test_Error': [gb_results[k]['test_error'] for k in gb_results.keys()],\n",
        "    'AUC': [gb_results[k]['auc'] for k in gb_results.keys()]\n",
        "})\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"GRADIENT BOOSTING MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(gb_comparison_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_depth = gb_comparison_df.loc[gb_comparison_df['Test_Error'].idxmin(), 'Max_Depth']\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Best Gradient Boosting Model: max_depth = {best_depth}\")\n",
        "print(f\"  Test Error: {gb_results[best_depth]['test_error']:.4f}\")\n",
        "print(f\"  Test Accuracy: {gb_results[best_depth]['test_accuracy']:.4f}\")\n",
        "print(f\"  AUC: {gb_results[best_depth]['auc']:.4f}\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d51a09e",
      "metadata": {
        "id": "6d51a09e"
      },
      "outputs": [],
      "source": [
        "# Visualize Gradient Boosting comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Confusion Matrix for Depth = 1\n",
        "ax = axes[0]\n",
        "cm = gb_results[1]['confusion_matrix']\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False,\n",
        "            annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
        "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Confusion Matrix: Depth = 1 (Stumps)\\nTest Error: {gb_results[1][\"test_error\"]:.4f}',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_xticklabels(['Inactive', 'Active'], fontsize=11)\n",
        "ax.set_yticklabels(['Inactive', 'Active'], fontsize=11, rotation=0)\n",
        "\n",
        "# Confusion Matrix for Depth = 2\n",
        "ax = axes[1]\n",
        "cm = gb_results[2]['confusion_matrix']\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=ax, cbar=False,\n",
        "            annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
        "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Confusion Matrix: Depth = 2\\nTest Error: {gb_results[2][\"test_error\"]:.4f}',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_xticklabels(['Inactive', 'Active'], fontsize=11)\n",
        "ax.set_yticklabels(['Inactive', 'Active'], fontsize=11, rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "906cca44",
      "metadata": {
        "id": "906cca44"
      },
      "source": [
        "### Gradient Boosting with Different Shrinkage Factors (Learning Rates)\n",
        "\n",
        "Now let's explore the effect of different shrinkage factors on Gradient Boosting performance.\n",
        "We'll use depth=2 (a good baseline) and vary the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73764101",
      "metadata": {
        "id": "73764101"
      },
      "outputs": [],
      "source": [
        "# Train Gradient Boosting with different shrinkage factors (learning rates)\n",
        "learning_rates = [0.01, 0.05, 0.1]\n",
        "gb_shrinkage_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"GRADIENT BOOSTING WITH DIFFERENT SHRINKAGE FACTORS\")\n",
        "print(\"Using max_depth=2, varying learning rate\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training Gradient Boosting with learning_rate = {lr}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Train Gradient Boosting , depth = 1\n",
        "    gb_model = GradientBoostingClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=1,\n",
        "        learning_rate=lr,\n",
        "        random_state=42,\n",
        "        subsample=0.8,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2\n",
        "    )\n",
        "\n",
        "    gb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = gb_model.predict(X_train)\n",
        "    y_test_pred = gb_model.predict(X_test)\n",
        "    y_test_proba = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    train_error = 1 - train_accuracy\n",
        "    test_error = 1 - test_accuracy\n",
        "    auc_score = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "    # Store results\n",
        "    gb_shrinkage_results[lr] = {\n",
        "        'model': gb_model,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'train_error': train_error,\n",
        "        'test_error': test_error,\n",
        "        'auc': auc_score,\n",
        "        'predictions': y_test_pred,\n",
        "        'probabilities': y_test_proba,\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_test_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Training Error:    {train_error:.4f}\")\n",
        "    print(f\"  Test Accuracy:     {test_accuracy:.4f}\")\n",
        "    print(f\"  Test Error:        {test_error:.4f}\")\n",
        "    print(f\"  AUC Score:         {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"All shrinkage factor models trained successfully!\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabb5f53",
      "metadata": {
        "id": "eabb5f53"
      },
      "outputs": [],
      "source": [
        "# Compare shrinkage factor models\n",
        "shrinkage_df = pd.DataFrame({\n",
        "    'Learning_Rate': list(gb_shrinkage_results.keys()),\n",
        "    'Train_Accuracy': [gb_shrinkage_results[k]['train_accuracy'] for k in gb_shrinkage_results.keys()],\n",
        "    'Test_Accuracy': [gb_shrinkage_results[k]['test_accuracy'] for k in gb_shrinkage_results.keys()],\n",
        "    'Train_Error': [gb_shrinkage_results[k]['train_error'] for k in gb_shrinkage_results.keys()],\n",
        "    'Test_Error': [gb_shrinkage_results[k]['test_error'] for k in gb_shrinkage_results.keys()],\n",
        "    'AUC': [gb_shrinkage_results[k]['auc'] for k in gb_shrinkage_results.keys()]\n",
        "})\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"GRADIENT BOOSTING: SHRINKAGE FACTOR COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(shrinkage_df.to_string(index=False))\n",
        "\n",
        "# Find best learning rate\n",
        "best_lr = shrinkage_df.loc[shrinkage_df['Test_Error'].idxmin(), 'Learning_Rate']\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Best Learning Rate: {best_lr}\")\n",
        "print(f\"  Test Error: {gb_shrinkage_results[best_lr]['test_error']:.4f}\")\n",
        "print(f\"  Test Accuracy: {gb_shrinkage_results[best_lr]['test_accuracy']:.4f}\")\n",
        "print(f\"  AUC: {gb_shrinkage_results[best_lr]['auc']:.4f}\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b882d7f8",
      "metadata": {
        "id": "b882d7f8"
      },
      "outputs": [],
      "source": [
        "# Visualize Shrinkage Factor Effect - Confusion Matrices for Different Learning Rates\n",
        "n_rates = len(learning_rates)\n",
        "fig, axes = plt.subplots(1, n_rates, figsize=(6*n_rates, 5))\n",
        "fig.suptitle('Gradient Boosting: Confusion Matrices for Different Learning Rates\\n(depth=2, 500 trees)',\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "colors_maps = ['Blues', 'Greens', 'Oranges']\n",
        "\n",
        "for idx, lr in enumerate(learning_rates):\n",
        "    ax = axes[idx]\n",
        "    cm = gb_shrinkage_results[lr]['confusion_matrix']\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=colors_maps[idx], ax=ax, cbar=False,\n",
        "                annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
        "\n",
        "    # Labels and title\n",
        "    ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'Learning Rate = {lr}\\nTest Error: {gb_shrinkage_results[lr][\"test_error\"]:.4f} | AUC: {gb_shrinkage_results[lr][\"auc\"]:.4f}',\n",
        "                fontsize=13, fontweight='bold', pad=10)\n",
        "    ax.set_xticklabels(['Inactive', 'Active'], fontsize=11)\n",
        "    ax.set_yticklabels(['Inactive', 'Active'], fontsize=11, rotation=0)\n",
        "\n",
        "    # Calculate and display accuracy metrics\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    # Add text box with metrics\n",
        "    textstr = f'Accuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}'\n",
        "    props = dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray')\n",
        "    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,\n",
        "            verticalalignment='top', bbox=props)\n",
        "\n",
        "    # Highlight best model\n",
        "    if lr == best_lr:\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_edgecolor('gold')\n",
        "            spine.set_linewidth(3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SHRINKAGE FACTOR ANALYSIS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best Learning Rate: {best_lr}\")\n",
        "print(f\"  - Lower learning rates (e.g., 0.01) require more trees but may generalize better\")\n",
        "print(f\"  - Higher learning rates (e.g., 0.1+) converge faster but may overfit\")\n",
        "print(f\"  - The optimal learning rate balances convergence speed and model performance\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515648d0",
      "metadata": {
        "id": "515648d0"
      },
      "source": [
        "### Test Error vs Number of Trees Comparison\n",
        "\n",
        "Compare how test classification error changes with the number of trees for different models:\n",
        "- **GB depth=1, lr=0.1**\n",
        "- **GB depth=2, lr=0.1**\n",
        "- **GB depth=1, lr=0.01**\n",
        "- **GB depth=1, lr=0.05**\n",
        "- **RF m=sqrt(p)=5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb0e7f3",
      "metadata": {
        "id": "8cb0e7f3"
      },
      "outputs": [],
      "source": [
        "# Train models with varying number of trees for comparison\n",
        "n_trees_list = [10, 25, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "error_comparison = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING MODELS WITH VARYING NUMBER OF TREES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Random Forest m = sqrt(p) = 5\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training Random Forest (m = sqrt(p) = 5) with varying trees...\")\n",
        "print(\"=\"*80)\n",
        "rf_errors = []\n",
        "for n_trees in n_trees_list:\n",
        "    rf_temp = RandomForestClassifier(\n",
        "        n_estimators=n_trees,\n",
        "        max_features=int(np.sqrt(n_features)),\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2\n",
        "    )\n",
        "    rf_temp.fit(X_train, y_train)\n",
        "    y_pred = rf_temp.predict(X_test)\n",
        "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
        "    rf_errors.append(test_error)\n",
        "    print(f\"  Trees: {n_trees:3d} | Test Error: {test_error:.4f}\")\n",
        "\n",
        "error_comparison['RF m=sqrt(p)=5'] = rf_errors\n",
        "\n",
        "# 2. GB depth=1, lr=0.1\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training GB (depth=1, lr=0.1) with varying trees...\")\n",
        "print(\"=\"*80)\n",
        "gb_d1_lr01_errors = []\n",
        "for n_trees in n_trees_list:\n",
        "    gb_temp = GradientBoostingClassifier(\n",
        "        n_estimators=n_trees,\n",
        "        max_depth=1,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        subsample=0.8,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2\n",
        "    )\n",
        "    gb_temp.fit(X_train, y_train)\n",
        "    y_pred = gb_temp.predict(X_test)\n",
        "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
        "    gb_d1_lr01_errors.append(test_error)\n",
        "    print(f\"  Trees: {n_trees:3d} | Test Error: {test_error:.4f}\")\n",
        "\n",
        "error_comparison['GB depth=1, lr=0.1'] = gb_d1_lr01_errors\n",
        "\n",
        "# 3. GB depth=2, lr=0.1\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training GB (depth=2, lr=0.1) with varying trees...\")\n",
        "print(\"=\"*80)\n",
        "gb_d2_lr01_errors = []\n",
        "for n_trees in n_trees_list:\n",
        "    gb_temp = GradientBoostingClassifier(\n",
        "        n_estimators=n_trees,\n",
        "        max_depth=2,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        subsample=0.8,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2\n",
        "    )\n",
        "    gb_temp.fit(X_train, y_train)\n",
        "    y_pred = gb_temp.predict(X_test)\n",
        "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
        "    gb_d2_lr01_errors.append(test_error)\n",
        "    print(f\"  Trees: {n_trees:3d} | Test Error: {test_error:.4f}\")\n",
        "\n",
        "error_comparison['GB depth=2, lr=0.1'] = gb_d2_lr01_errors\n",
        "\n",
        "# 4. GB depth=1, lr=0.01\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training GB (depth=1, lr=0.01) with varying trees...\")\n",
        "print(\"=\"*80)\n",
        "gb_d1_lr001_errors = []\n",
        "for n_trees in n_trees_list:\n",
        "    gb_temp = GradientBoostingClassifier(\n",
        "        n_estimators=n_trees,\n",
        "        max_depth=1,\n",
        "        learning_rate=0.01,\n",
        "        random_state=42,\n",
        "        subsample=0.8,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2\n",
        "    )\n",
        "    gb_temp.fit(X_train, y_train)\n",
        "    y_pred = gb_temp.predict(X_test)\n",
        "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
        "    gb_d1_lr001_errors.append(test_error)\n",
        "    print(f\"  Trees: {n_trees:3d} | Test Error: {test_error:.4f}\")\n",
        "\n",
        "error_comparison['GB depth=1, lr=0.01'] = gb_d1_lr001_errors\n",
        "\n",
        "# 5. GB depth=1, lr=0.05\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training GB (depth=1, lr=0.05) with varying trees...\")\n",
        "print(\"=\"*80)\n",
        "gb_d1_lr005_errors = []\n",
        "for n_trees in n_trees_list:\n",
        "    gb_temp = GradientBoostingClassifier(\n",
        "        n_estimators=n_trees,\n",
        "        max_depth=1,\n",
        "        learning_rate=0.05,\n",
        "        random_state=42,\n",
        "        subsample=0.8,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2\n",
        "    )\n",
        "    gb_temp.fit(X_train, y_train)\n",
        "    y_pred = gb_temp.predict(X_test)\n",
        "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
        "    gb_d1_lr005_errors.append(test_error)\n",
        "    print(f\"  Trees: {n_trees:3d} | Test Error: {test_error:.4f}\")\n",
        "\n",
        "error_comparison['GB depth=1, lr=0.05'] = gb_d1_lr005_errors\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"All models trained successfully!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80c985d",
      "metadata": {
        "id": "a80c985d"
      },
      "outputs": [],
      "source": [
        "# Plot Test Error vs Number of Trees\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "colors = ['steelblue', 'coral', 'forestgreen', 'purple', 'orange']\n",
        "markers = ['o', 's', '^', 'D', 'v']\n",
        "linestyles = ['-', '-', '-', '--', '--']\n",
        "\n",
        "for idx, (model_name, errors) in enumerate(error_comparison.items()):\n",
        "    plt.plot(n_trees_list, errors,\n",
        "             marker=markers[idx],\n",
        "             linewidth=2.5,\n",
        "             markersize=8,\n",
        "             color=colors[idx],\n",
        "             linestyle=linestyles[idx],\n",
        "             label=model_name,\n",
        "             alpha=0.8)\n",
        "\n",
        "plt.xlabel('Number of Trees', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Test Classification Error', fontsize=14, fontweight='bold')\n",
        "plt.title('Test Error vs Number of Trees: Model Comparison', fontsize=16, fontweight='bold')\n",
        "plt.legend(fontsize=11, loc='upper right')\n",
        "plt.grid(alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY: Final Test Errors at 500 Trees\")\n",
        "print(\"=\"*80)\n",
        "for model_name, errors in error_comparison.items():\n",
        "    final_error = errors[-1]\n",
        "    initial_error = errors[0]\n",
        "    improvement = (initial_error - final_error) * 100\n",
        "    print(f\"{model_name:30s} | Final Error: {final_error:.4f} | Improvement: {improvement:.2f}%\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY INSIGHTS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"1. Learning Rate Effect (GB depth=1):\")\n",
        "print(\"   - lr=0.01: Slowest convergence, may need more trees\")\n",
        "print(\"   - lr=0.05: Moderate convergence\")\n",
        "print(\"   - lr=0.1:  Fastest convergence\")\n",
        "print(\"\\n2. Depth Effect (GB with lr=0.1):\")\n",
        "print(\"   - depth=1 (stumps): Simple weak learners\")\n",
        "print(\"   - depth=2: Better performance, more complex trees\")\n",
        "print(\"\\n3. Random Forest:\")\n",
        "print(\"   - Parallel training, different convergence pattern\")\n",
        "print(\"   - Performance stabilizes around 200-300 trees\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ab249fc",
      "metadata": {
        "id": "7ab249fc"
      },
      "source": [
        "### Focused Comparison: Gradient Boosting vs Random Forest\n",
        "\n",
        "Let's compare specific models:\n",
        "- **Gradient Boosting**: depths 1, 2, and 5\n",
        "- **Random Forest**: m = 5 and m = sqrt(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956df56b",
      "metadata": {
        "id": "956df56b"
      },
      "source": [
        "## 7. Overall Model Comparison and Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4afeebf8",
      "metadata": {
        "id": "4afeebf8"
      },
      "outputs": [],
      "source": [
        "# Compare all models (Random Forest and Gradient Boosting)\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL MODEL COMPARISON: RANDOM FOREST VS GRADIENT BOOSTING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_models = []\n",
        "\n",
        "# Add Random Forest models\n",
        "for name in rf_results.keys():\n",
        "    all_models.append({\n",
        "        'Model Type': 'Random Forest',\n",
        "        'Configuration': name,\n",
        "        'Test Error': rf_results[name]['test_error'],\n",
        "        'Test Accuracy': rf_results[name]['test_accuracy'],\n",
        "        'AUC': rf_results[name]['auc']\n",
        "    })\n",
        "\n",
        "# Add Gradient Boosting models (main depth variations)\n",
        "for depth in gb_results.keys():\n",
        "    all_models.append({\n",
        "        'Model Type': 'Gradient Boosting',\n",
        "        'Configuration': f'depth={depth}, lr=0.1',\n",
        "        'Test Error': gb_results[depth]['test_error'],\n",
        "        'Test Accuracy': gb_results[depth]['test_accuracy'],\n",
        "        'AUC': gb_results[depth]['auc']\n",
        "    })\n",
        "\n",
        "# Add Gradient Boosting models with different learning rates (depth=1)\n",
        "for lr in gb_shrinkage_results.keys():\n",
        "    all_models.append({\n",
        "        'Model Type': 'Gradient Boosting',\n",
        "        'Configuration': f'depth=1, lr={lr}',\n",
        "        'Test Error': gb_shrinkage_results[lr]['test_error'],\n",
        "        'Test Accuracy': gb_shrinkage_results[lr]['test_accuracy'],\n",
        "        'AUC': gb_shrinkage_results[lr]['auc']\n",
        "    })\n",
        "\n",
        "final_comparison_df = pd.DataFrame(all_models)\n",
        "final_comparison_df = final_comparison_df.sort_values('Test Error')\n",
        "\n",
        "print(final_comparison_df.to_string(index=False))\n",
        "\n",
        "# Find overall best model\n",
        "best_idx = final_comparison_df['Test Error'].idxmin()\n",
        "best_overall = final_comparison_df.loc[best_idx]\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"OVERALL BEST MODEL:\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Model Type:     {best_overall['Model Type']}\")\n",
        "print(f\"Configuration:  {best_overall['Configuration']}\")\n",
        "print(f\"Test Error:     {best_overall['Test Error']:.4f}\")\n",
        "print(f\"Test Accuracy:  {best_overall['Test Accuracy']:.4f}\")\n",
        "print(f\"AUC:            {best_overall['AUC']:.4f}\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "127440fa",
      "metadata": {
        "id": "127440fa"
      },
      "outputs": [],
      "source": [
        "# Feature Importance Analysis\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get feature importance from best Random Forest model\n",
        "best_rf_model = rf_results[best_model_name]['model']\n",
        "feature_importance_rf = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': best_rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Get feature importance from best Gradient Boosting model\n",
        "best_gb_model = gb_results[best_depth]['model']\n",
        "feature_importance_gb = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': best_gb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 15 Most Important Features (Random Forest - {best_model_name}):\")\n",
        "print(\"=\"*80)\n",
        "print(feature_importance_rf.head(15).to_string(index=False))\n",
        "\n",
        "print(f\"\\n\\nTop 15 Most Important Features (Gradient Boosting - depth={best_depth}):\")\n",
        "print(\"=\"*80)\n",
        "print(feature_importance_gb.head(15).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "046932ce",
      "metadata": {
        "id": "046932ce"
      },
      "outputs": [],
      "source": [
        "# Bagging Model (m = p) - Detailed Feature Importance Analysis\n",
        "print(\"=\"*80)\n",
        "print(\"BAGGING MODEL FEATURE IMPORTANCE (Gini Impurity Reduction)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get bagging model (m = p)\n",
        "bagging_model = rf_results['m = p (Bagging)']['model']\n",
        "bagging_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': bagging_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f\"\\nMethod: Mean Decrease in Impurity (MDI) - Gini Reduction\")\n",
        "print(f\"Description: Total reduction in Gini impurity across all {bagging_model.n_estimators} trees\")\n",
        "print(f\"              weighted by the probability of reaching each node\\n\")\n",
        "\n",
        "print(\"Top 20 Most Influential Features:\")\n",
        "print(\"-\" * 80)\n",
        "print(bagging_importance.head(20).to_string(index=False))\n",
        "\n",
        "# Visualize relative influence for bagging model\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Top 20 features\n",
        "ax = axes[0]\n",
        "top_20 = bagging_importance.head(20)\n",
        "ax.barh(range(len(top_20)), top_20['Importance'], color='#45b7d1', alpha=0.8, edgecolor='black')\n",
        "ax.set_yticks(range(len(top_20)))\n",
        "ax.set_yticklabels(top_20['Feature'], fontsize=10)\n",
        "ax.set_xlabel('Relative Influence (Gini Reduction)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Bagging Model: Top 20 Feature Importance\\n(m = p, using all features)',\n",
        "             fontsize=13, fontweight='bold')\n",
        "ax.invert_yaxis()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Add percentage labels\n",
        "for i, (idx, row) in enumerate(top_20.iterrows()):\n",
        "    pct = row['Importance'] / bagging_importance['Importance'].sum() * 100\n",
        "    ax.text(row['Importance'], i, f' {pct:.1f}%', va='center', fontsize=8)\n",
        "\n",
        "# Cumulative importance\n",
        "ax = axes[1]\n",
        "cumsum = bagging_importance['Importance'].cumsum()\n",
        "cumsum_pct = (cumsum / bagging_importance['Importance'].sum() * 100)\n",
        "ax.plot(range(1, len(cumsum_pct) + 1), cumsum_pct, linewidth=2.5, color='#45b7d1')\n",
        "ax.axhline(y=80, color='red', linestyle='--', linewidth=2, label='80% threshold')\n",
        "ax.axhline(y=90, color='orange', linestyle='--', linewidth=2, label='90% threshold')\n",
        "ax.set_xlabel('Number of Features', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Cumulative Importance (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Bagging Model: Cumulative Feature Importance', fontsize=13, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "ax.legend()\n",
        "ax.set_ylim([0, 105])\n",
        "\n",
        "# Find features needed for 80% and 90%\n",
        "features_80 = (cumsum_pct >= 80).argmax() + 1\n",
        "features_90 = (cumsum_pct >= 90).argmax() + 1\n",
        "ax.axvline(x=features_80, color='red', linestyle=':', alpha=0.5)\n",
        "ax.axvline(x=features_90, color='orange', linestyle=':', alpha=0.5)\n",
        "ax.text(features_80, 5, f'{features_80} features', rotation=90, va='bottom', ha='right', fontsize=9)\n",
        "ax.text(features_90, 5, f'{features_90} features', rotation=90, va='bottom', ha='right', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"FEATURE IMPORTANCE SUMMARY:\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Top 5 features explain: {cumsum_pct.iloc[4]:.1f}% of total importance\")\n",
        "print(f\"Top 10 features explain: {cumsum_pct.iloc[9]:.1f}% of total importance\")\n",
        "print(f\"Top 20 features explain: {cumsum_pct.iloc[19]:.1f}% of total importance\")\n",
        "print(f\"{features_80} features needed for 80% cumulative importance\")\n",
        "print(f\"{features_90} features needed for 90% cumulative importance\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db5d2679",
      "metadata": {
        "id": "db5d2679"
      },
      "source": [
        "## 8. Key Findings and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "029f85af",
      "metadata": {
        "id": "029f85af"
      },
      "source": [
        "## Comprehensive Summary of Key Findings\n",
        "\n",
        "### 1. Dataset Overview\n",
        "- **Total Records**: Netfix simulate data\n",
        "- **Features**: 30 features after engineering (demographics, subscription, engagement, content)\n",
        "- **Target Distribution**: Binary classification (active vs inactive users)\n",
        "- **Train/Test Split**: 80/20 with stratification\n",
        "\n",
        "### 2. Random Forest Performance (500 trees)\n",
        "\n",
        "**Three configurations tested:**\n",
        "- **m = sqrt(p) ≈ 5**: Default classification setting\n",
        "- **m = p/2 = 15**: Moderate feature selection\n",
        "- **m = p = 30 (Bagging)**: Uses all features\n",
        "\n",
        "**Key Findings:**\n",
        "- All three RF models showed strong performance (test error typically < 5%)\n",
        "- **m = sqrt(p)** provides good balance between variance reduction and tree diversity\n",
        "- **m = p/2** offers middle ground with reasonable decorrelation\n",
        "- **m = p (Bagging)** uses all features but may have higher tree correlation\n",
        "- Models converge around 200-300 trees, with diminishing returns after 400 trees\n",
        "\n",
        "### 3. Gradient Boosting Performance\n",
        "\n",
        "**Depth Analysis (learning rate = 0.1, 500 trees):**\n",
        "- **Depth = 1 (Stumps)**: Simplest weak learners, may underfit\n",
        "- **Depth = 2**: Good balance between complexity and generalization\n",
        "\n",
        "**Key Findings:**\n",
        "- Sequential learning allows GB to correct previous errors\n",
        "- Depth = 2 often provides optimal bias-variance tradeoff\n",
        "- Deeper trees (depth = 5) achieve lower training error but need careful monitoring\n",
        "\n",
        "### 4. Shrinkage Factor (Learning Rate) Analysis\n",
        "\n",
        "**Tested learning rates (depth = 1, 500 trees):**\n",
        "- **lr = 0.01**: Very slow learning, requires many trees, most conservative\n",
        "- **lr = 0.05**: Moderate convergence speed, balanced approach\n",
        "- **lr = 0.1**: Faster convergence, standard default value\n",
        "\n",
        "**Key Findings:**\n",
        "- **Lower learning rates** (0.01-0.05):\n",
        "  - Slower convergence but potentially better generalization\n",
        "  - Require more trees to achieve optimal performance\n",
        "  - More resistant to overfitting\n",
        "  \n",
        "- **Higher learning rates** (0.1+):\n",
        "  - Faster convergence (fewer trees needed)\n",
        "  - Risk of overshooting optimal solution\n",
        "  - May overfit with too many trees\n",
        "\n",
        "- **Optimal strategy**: Balance between learning rate and number of trees\n",
        "  - lr=0.1 with 300-500 trees often works well\n",
        "  - lr=0.01 may need 1000+ trees for full convergence\n",
        "\n",
        "### 5. Random Forest vs Gradient Boosting\n",
        "\n",
        "**Random Forest Advantages:**\n",
        "- Parallel training (faster with multiple cores)\n",
        "- Less sensitive to hyperparameters\n",
        "- Built-in feature importance via Gini reduction\n",
        "- Naturally handles overfitting through averaging\n",
        "- More stable predictions (less variance)\n",
        "\n",
        "**Gradient Boosting Advantages:**\n",
        "- Sequential error correction (often higher accuracy)\n",
        "- More flexible with depth and learning rate tuning\n",
        "- Can achieve better performance with proper tuning\n",
        "- Better at capturing complex interactions\n",
        "\n",
        "**Performance Comparison:**\n",
        "- GB with optimal settings typically edges out RF in test accuracy\n",
        "- RF provides more consistent results across different m values\n",
        "- GB requires more careful hyperparameter tuning\n",
        "\n",
        "### 6. Convergence Analysis: Test Error vs Number of Trees\n",
        "\n",
        "**Models compared across 10-500 trees:**\n",
        "1. RF m=sqrt(p)=5\n",
        "2. GB depth=1, lr=0.1\n",
        "3. GB depth=2, lr=0.1\n",
        "4. GB depth=1, lr=0.01\n",
        "5. GB depth=1, lr=0.05\n",
        "\n",
        "**Key Findings:**\n",
        "- **GB with lr=0.1**: Fastest convergence (stable by 100-150 trees)\n",
        "- **GB with lr=0.05**: Moderate convergence (stable by 200-250 trees)\n",
        "- **GB with lr=0.01**: Slowest convergence (may need 500+ trees)\n",
        "- **RF m=sqrt(p)**: Steady convergence, stable by 250-300 trees\n",
        "- **GB depth=2 vs depth=1**: Deeper trees achieve lower error faster\n",
        "\n",
        "**Practical Implications:**\n",
        "- For quick prototyping: Use GB with lr=0.1 and 100-200 trees\n",
        "- For production: Use lr=0.05 with 300-500 trees for better generalization\n",
        "- RF offers good \"out-of-the-box\" performance without extensive tuning\n",
        "\n",
        "### 7. Feature Importance Insights\n",
        "\n",
        "**Top predictive features consistently included:**\n",
        "- User engagement metrics (watch time, session count)\n",
        "- Content interaction (recommendations clicked, searches)\n",
        "- Subscription characteristics (plan type, monthly spend)\n",
        "- Temporal patterns (subscription duration, recency)\n",
        "- Demographics (age, household composition)\n",
        "\n",
        "**Model-specific insights:**\n",
        "- RF: More democratic feature importance (flatter distribution)\n",
        "- GB: More concentrated importance on top features (steeper distribution)\n",
        "- Both methods agree on top 5-10 most important features\n",
        "\n",
        "### 8. Model Selection Recommendations\n",
        "\n",
        "**Choose Random Forest when:**\n",
        "- Need fast, parallel training\n",
        "- Want stable, interpretable results\n",
        "- Have limited time for hyperparameter tuning\n",
        "- Prefer robust performance across datasets\n",
        "\n",
        "**Choose Gradient Boosting when:**\n",
        "- Need maximum predictive accuracy\n",
        "- Have time for careful hyperparameter tuning\n",
        "- Can afford sequential training time\n",
        "- Want to fine-tune bias-variance tradeoff\n",
        "\n",
        "**Hyperparameter Guidelines:**\n",
        "- **RF**: Start with m=sqrt(p), 300-500 trees\n",
        "- **GB**: Start with depth=2, lr=0.1, 200-300 trees\n",
        "- **Fine-tuning**: Adjust based on validation error curves\n",
        "\n",
        "### 9. Business Implications for Netflix\n",
        "\n",
        "**Churn Prediction:**\n",
        "- Models achieve high accuracy (>95%) in predicting user activity status\n",
        "- Early warning system possible for users at risk of becoming inactive\n",
        "- Key intervention points identified through feature importance\n",
        "\n",
        "**Actionable Insights:**\n",
        "1. **Engagement monitoring**: Track watch time and session frequency\n",
        "2. **Recommendation optimization**: Improve click-through rates\n",
        "3. **Personalization**: Target content based on viewing patterns\n",
        "4. **Retention campaigns**: Focus on users with declining engagement metrics\n",
        "5. **Subscription optimization**: Address plan-specific churn patterns\n",
        "\n",
        "### 10. Technical Conclusions\n",
        "\n",
        "**Model Performance Summary:**\n",
        "- Both RF and GB achieve excellent performance (test error 3-5%)\n",
        "- Ensemble methods vastly outperform single decision trees\n",
        "- Proper hyperparameter tuning yields 1-2% improvement\n",
        "- Learning rate is critical for GB convergence and generalization\n",
        "\n",
        "**Best Practices Identified:**\n",
        "1. Always validate on separate test set\n",
        "2. Monitor train vs test error to detect overfitting\n",
        "3. Use cross-validation for final model selection\n",
        "4. Consider computational constraints when choosing method\n",
        "5. Balance model complexity with interpretability needs\n",
        "\n",
        "**Final Model Recommendation:**\n",
        "- **For deployment**: GB with depth=2, lr=0.1, 300 trees\n",
        "- **For interpretability**: RF with m=sqrt(p), 400 trees\n",
        "- **For experimentation**: Test both and ensemble predictions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env8222a",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}